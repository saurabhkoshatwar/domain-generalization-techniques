# domain-generalization-techniques

#### Domain generalization is the process of learning a model from one or more distinct but related domains (i.e., diverse training datasets that will generalize well to previously unseen testing domains. For example, given a training set of images derived from sketches, cartoons, and paintings

#### Several fields of study are closely related to domainÂ generalization, including but not limited to -

 <p>
 <a href="https://github.com/saurabhkoshatwar/NSMDA-FinalYearProject">
  <img align="right" src="https://github-readme-stats.vercel.app/api/pin/?username=saurabhkoshatwar&repo=Domain-adaptation-deep-learning&title_color=ffffff&text_color=c9cacc&icon_color=2bbc8a&bg_color=1d1f21" />
</a>
</p>

* #### Domain adaptation
Domain adaptation aims to maximize the performance on a given target domain using existing training source domain

* #### Transfer learning
Transfer learning trains a model on a source task with the goal of improving the model's performance on a different but related target domain/task. Pretraining-finetuning is a common transfer learning strategy in which the source and target domains have different tasks and the target domain is accessed during training.

* #### Multi-task learning
Multi-task learning optimizes models on multiple related tasks at the same time. We could improve the model's generalization on the efforts exerted by sharing representations between these tasks.

* #### Meta-learning
Meta-learning seeks to learn the learning algorithm itself through prior experience or tasks, i.e., learning-to-learn.
